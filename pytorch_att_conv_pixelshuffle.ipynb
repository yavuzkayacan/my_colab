{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+jcgpUDIhaJACSekloIZG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yavuzkayacan/my_colab/blob/main/pytorch_att_conv_pixelshuffle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xqb81w5juDFn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import einsum\n",
        "from torch import Tensor\n",
        "import math\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "igGQ5qvwvS_E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(nn.Module):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "\n",
        "    def forward(self, images):\n",
        "        batch_size, channels, height, width = images.size()\n",
        "\n",
        "        patches = images.unfold(2, self.patch_size[0], self.patch_size[0]).unfold(3, self.patch_size[1], self.patch_size[1])\n",
        "        patches = patches.contiguous().view(batch_size, channels, -1, self.patch_size[0], self.patch_size[1])\n",
        "\n",
        "\n",
        "        return patches"
      ],
      "metadata": {
        "id": "f-TRyxfQvTlv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchComb(nn.Module):\n",
        "\n",
        "    def __init__(self, patch_size, num_patches, ch):\n",
        "        super(PatchComb, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.ch = ch\n",
        "\n",
        "    def forward(self,patches):\n",
        "        #reshaped_patches = patches.shape\n",
        "        reshaped_patches = torch.reshape(patches, (-1, self.num_patches[0], self.num_patches[1], self.patch_size[0], self.patch_size[1], self.ch))\n",
        "        #print(reshaped_patches)\n",
        "        #reshaped_patches = reshaped_patches.permute(0, 1, 3, 2, 4, 5)\n",
        "        reshaped_patches = reshaped_patches.permute(0, 5, 1, 3, 2, 4)\n",
        "        #print(reshaped_patches.shape)\n",
        "        reshaped_patches = torch.reshape(reshaped_patches, (-1, self.ch, self.num_patches[0]*self.patch_size[0], self.num_patches[1]*self.patch_size[1]))\n",
        "        return reshaped_patches"
      ],
      "metadata": {
        "id": "4EAt7o6zvhvS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvAttention(nn.Module):\n",
        "    def __init__(self, n_channels):\n",
        "        super(ConvAttention, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.query = nn.Conv3d(self.n_channels, self.n_channels, kernel_size=3, padding='same')\n",
        "        self.key = nn.Conv3d(self.n_channels, self.n_channels, kernel_size=3, padding='same')\n",
        "        self.value = nn.Conv3d(self.n_channels, self.n_channels, kernel_size=3, padding='same')\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.shape\n",
        "        f, g, h = self.query(x), self.key(x), self.value(x)\n",
        "        mat_mul = torch.matmul(f.permute(0, 1, 2, 3, 4), g)\n",
        "        beta = F.softmax(mat_mul, dim=1)\n",
        "        o = self.gamma * torch.matmul(h, beta) + x\n",
        "\n",
        "        return mat_mul"
      ],
      "metadata": {
        "id": "M5lJjddbvj5P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand((1,16,256,64))"
      ],
      "metadata": {
        "id": "UFvI2ywGvnv4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (1, 256, 64)\n",
        "patch_size = [4, 4]\n",
        "num_patches = (input_shape[1] // patch_size[0]) * (input_shape[2] // patch_size[1])\n",
        "projection_dim = 64\n",
        "num_patch = np.array(((input_shape[1] // patch_size[0]) , (input_shape[2] // patch_size[1])))"
      ],
      "metadata": {
        "id": "xErCcUdwvvUp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ch_in = 1"
      ],
      "metadata": {
        "id": "iavT1TkPv9hN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = nn.Conv2d(X.shape[1], ch_in ,kernel_size=3, padding='same')(X)"
      ],
      "metadata": {
        "id": "LDOEKd4PvxyL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnPW2I9Mv6DH",
        "outputId": "1005bb31-4fc6-4151-fad3-4b237d9cc256"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peC17ASu5ARN",
        "outputId": "0acef9fd-d353-4a46-b0fe-b07ec8b41802"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = Patches(patch_size)(X)"
      ],
      "metadata": {
        "id": "rdSUMbM3wAct"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXdIDPCRwDLp",
        "outputId": "cecd9625-ca89-4d2a-b8ff-fe407b614a84"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 1024, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = ConvAttention(ch_in)(X)\n",
        "X = PatchComb(patch_size, num_patch, ch_in)(X)"
      ],
      "metadata": {
        "id": "QbfFOn1EwEIT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDIW4a0-wHxB",
        "outputId": "386262e3-4b8a-47e6-b826-8141536b21b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.PixelUnshuffle(2)(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf3IYek9wJI5",
        "outputId": "592ee431-971d-469d-a2fe-1eb8a091c795"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 128, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv_Transformers(nn.Module):\n",
        "    def __init__(self, patch_size, num_patch, in_ch, ch, depth):\n",
        "        super(Conv_Transformers, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patch = num_patch\n",
        "        self.in_ch = in_ch\n",
        "        self.ch = ch\n",
        "\n",
        "        self.depth = depth\n",
        "\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.in_ch, out_channels=self.ch, kernel_size=1,padding='same')\n",
        "\n",
        "        self.patching = Patches(self.patch_size)  # Assuming Patches is defined\n",
        "        self.conv_att = ConvAttention(self.ch)  # Assuming ConvAttention is defined\n",
        "        self.patch_comb = PatchComb(self.patch_size, self.num_patch, self.ch)  # Assuming PatchComb is defined\n",
        "\n",
        "        self.ds = nn.PixelUnshuffle(2)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X = self.conv(X)\n",
        "        X = self.patching(X)\n",
        "        X = self.conv_att(X)\n",
        "        X = self.patch_comb(X)\n",
        "        X = self.ds(X)\n",
        "\n",
        "\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "BjbVRU5wzipr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_patch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KsyCfQm4B1s",
        "outputId": "f7b0ec11-2152-4fcf-bc28-4f6f0eb7c584"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([64, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8epJsivZ7VkD",
        "outputId": "0a568bf8-b203-405a-bcc7-5321d7873203"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Conv_Transformers(patch_size=patch_size,ch=64,in_ch=1,num_patch=num_patch,depth=1)(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLsrSr4c2-J8",
        "outputId": "f7adea68-6ee7-4f6b-cd73-74d4c6e951e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 128, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv_Transformers(nn.Module):\n",
        "    def __init__(self, patch_size, num_patch, in_ch, ch, depth):\n",
        "        super(Conv_Transformers, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patch = num_patch\n",
        "        self.in_ch = in_ch\n",
        "        self.ch = ch\n",
        "\n",
        "        self.depth = depth\n",
        "\n",
        "        self.layers_down = nn.ModuleList()\n",
        "\n",
        "        self.layers_up = nn.ModuleList()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.in_ch, out_channels=self.ch, kernel_size=1,padding='same')\n",
        "\n",
        "        self.down_ch = []\n",
        "\n",
        "        for _ in range(depth):\n",
        "\n",
        "\n",
        "          self.layers_down.append(nn.Sequential(\n",
        "          Patches(self.patch_size),  # Assuming Patches is defined\n",
        "          ConvAttention(self.ch),  # Assuming ConvAttention is defined\n",
        "          PatchComb(self.patch_size, self.num_patch, self.ch),  # Assuming PatchComb is defined\n",
        "          nn.PixelUnshuffle(2)\n",
        "          ))\n",
        "\n",
        "          self.down_ch.append(self.ch)\n",
        "          self.ch = self.ch*4\n",
        "          self.num_patch =  self.num_patch // 2\n",
        "\n",
        "\n",
        "\n",
        "        self.down_ch = self.down_ch[::-1]\n",
        "        self.a = 0\n",
        "\n",
        "\n",
        "        for _ in range(depth):\n",
        "\n",
        "\n",
        "\n",
        "          self.layers_up.append(nn.Sequential(\n",
        "          Patches(self.patch_size),  # Assuming Patches is defined\n",
        "          ConvAttention(self.ch),  # Assuming ConvAttention is defined\n",
        "          PatchComb(self.patch_size, self.num_patch, self.ch),  # Assuming PatchComb is defined\n",
        "          nn.PixelShuffle(2)\n",
        "          ))\n",
        "\n",
        "          #if(self.a==0):\n",
        "          self.ch = self.ch//4 + self.down_ch[_]\n",
        "          #else:\n",
        "            #self.ch = self.ch//4\n",
        "\n",
        "          self.num_patch =  self.num_patch * 2\n",
        "\n",
        "          self.a=self.a+1\n",
        "          #print(self.a)\n",
        "          #print(self.ch)\n",
        "          #print(self.down_ch[_])\n",
        "\n",
        "    def forward(self,X):\n",
        "        X_skip = []\n",
        "        #print(X.shape)\n",
        "        X = self.conv(X)\n",
        "        #print(X.shape)\n",
        "        for i in range(self.depth):\n",
        "            X_skip.append(X)\n",
        "            X = self.layers_down[i](X)\n",
        "            #print(X.shape)\n",
        "\n",
        "        X_skip = X_skip[::-1]\n",
        "\n",
        "        for i in range(self.depth):\n",
        "\n",
        "            X = self.layers_up[i](X)\n",
        "            #print(X.shape)\n",
        "\n",
        "            X_con = torch.concat([X,X_skip[i]],1)\n",
        "            #print(\"concat_shape: \" + str(X_con.shape))\n",
        "\n",
        "            X = X_con\n",
        "            #self.ch = X.shape[1]\n",
        "\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "sSQNbyx03PdC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_s = Conv_Transformers(patch_size=patch_size,ch=4,in_ch=1,num_patch=num_patch,depth=3)(X)"
      ],
      "metadata": {
        "id": "mDn4ztjm85lz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Conv_Transformers(patch_size=patch_size,ch=4,in_ch=1,num_patch=num_patch,depth=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1XL5rLB7tzL",
        "outputId": "b9097065-cd5b-417b-deef-6e92a84fc1aa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv_Transformers(\n",
              "  (layers_down): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (0): Patches()\n",
              "      (1): ConvAttention(\n",
              "        (query): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (key): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (value): Conv3d(4, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "      )\n",
              "      (2): PatchComb()\n",
              "      (3): PixelUnshuffle(downscale_factor=2)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Patches()\n",
              "      (1): ConvAttention(\n",
              "        (query): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (key): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (value): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "      )\n",
              "      (2): PatchComb()\n",
              "      (3): PixelUnshuffle(downscale_factor=2)\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Patches()\n",
              "      (1): ConvAttention(\n",
              "        (query): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (key): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (value): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "      )\n",
              "      (2): PatchComb()\n",
              "      (3): PixelUnshuffle(downscale_factor=2)\n",
              "    )\n",
              "  )\n",
              "  (layers_up): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (0): Patches()\n",
              "      (1): ConvAttention(\n",
              "        (query): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (key): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (value): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "      )\n",
              "      (2): PatchComb()\n",
              "      (3): PixelShuffle(upscale_factor=2)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Patches()\n",
              "      (1): ConvAttention(\n",
              "        (query): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (key): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (value): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "      )\n",
              "      (2): PatchComb()\n",
              "      (3): PixelShuffle(upscale_factor=2)\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Patches()\n",
              "      (1): ConvAttention(\n",
              "        (query): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (key): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "        (value): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same)\n",
              "      )\n",
              "      (2): PatchComb()\n",
              "      (3): PixelShuffle(upscale_factor=2)\n",
              "    )\n",
              "  )\n",
              "  (conv): Conv2d(1, 4, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_s.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orL7oBZ46_eZ",
        "outputId": "ea92fd8d-8a79-4af7-a2f5-a9cef99f7c98"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 20, 256, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_s[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cafb641z8-VP",
        "outputId": "d7944585-76dc-49db-ec51-9c10e72ea02f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 256, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_s[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ-u0AvyPgbc",
        "outputId": "3ea482c1-6cc0-4bcc-9d55-e167321635b5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 128, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_s[2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ee9V5IHPjfb",
        "outputId": "ef6b21ff-cf45-47e5-8c68-f98c898e4360"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 256, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EoYQRlk5PmQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}